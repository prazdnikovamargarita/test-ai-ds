{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 259ms/step - accuracy: 0.8472 - loss: 0.8318 - val_accuracy: 0.9943 - val_loss: 0.0537\n",
      "Epoch 2/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 176ms/step - accuracy: 0.9944 - loss: 0.0471 - val_accuracy: 0.9943 - val_loss: 0.0384\n",
      "Epoch 3/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 191ms/step - accuracy: 0.9944 - loss: 0.0359 - val_accuracy: 0.9943 - val_loss: 0.0347\n",
      "Epoch 4/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 210ms/step - accuracy: 0.9946 - loss: 0.0316 - val_accuracy: 0.9943 - val_loss: 0.0317\n",
      "Epoch 5/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 197ms/step - accuracy: 0.9946 - loss: 0.0286 - val_accuracy: 0.9943 - val_loss: 0.0278\n",
      "Epoch 6/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 203ms/step - accuracy: 0.9945 - loss: 0.0251 - val_accuracy: 0.9943 - val_loss: 0.0233\n",
      "Epoch 7/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 235ms/step - accuracy: 0.9946 - loss: 0.0205 - val_accuracy: 0.9943 - val_loss: 0.0192\n",
      "Epoch 8/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 229ms/step - accuracy: 0.9946 - loss: 0.0169 - val_accuracy: 0.9943 - val_loss: 0.0170\n",
      "Epoch 9/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 206ms/step - accuracy: 0.9946 - loss: 0.0152 - val_accuracy: 0.9948 - val_loss: 0.0158\n",
      "Epoch 10/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 219ms/step - accuracy: 0.9952 - loss: 0.0142 - val_accuracy: 0.9948 - val_loss: 0.0151\n",
      "Epoch 11/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 207ms/step - accuracy: 0.9950 - loss: 0.0144 - val_accuracy: 0.9950 - val_loss: 0.0150\n",
      "Epoch 12/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 199ms/step - accuracy: 0.9954 - loss: 0.0136 - val_accuracy: 0.9949 - val_loss: 0.0146\n",
      "Epoch 13/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 199ms/step - accuracy: 0.9954 - loss: 0.0138 - val_accuracy: 0.9951 - val_loss: 0.0143\n",
      "Epoch 14/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 211ms/step - accuracy: 0.9955 - loss: 0.0134 - val_accuracy: 0.9949 - val_loss: 0.0139\n",
      "Epoch 15/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 205ms/step - accuracy: 0.9956 - loss: 0.0126 - val_accuracy: 0.9950 - val_loss: 0.0130\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load dataset\n",
    "with open(\"animal_ner_english_dataset.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract sentences and corresponding tags\n",
    "sentences_list = [item[\"sentence\"] for item in data]\n",
    "labels_list = [item[\"category\"] for item in data]\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences_list)\n",
    "vocab = tokenizer.word_index\n",
    "VOCAB_SIZE = len(vocab) + 1  # Add 1 for padding\n",
    "\n",
    "# Create tag mapping\n",
    "unique_labels = sorted(set(tag for tags in labels_list for tag in tags))\n",
    "if \"O\" not in unique_labels:\n",
    "    unique_labels.append(\"O\")  # Ensure \"O\" exists\n",
    "tag_map = {label: i for i, label in enumerate(unique_labels)}\n",
    "NUM_CLASSES = len(tag_map)\n",
    "\n",
    "# Hyperparameters\n",
    "MAX_LEN = 100\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "EMBEDDING_DIM = 100\n",
    "RNN_UNITS = 100\n",
    "\n",
    "\n",
    "# Function to prepare data\n",
    "def get_params(sentences_list, labels_list):\n",
    "    tokenized_sentences = tokenizer.texts_to_sequences(sentences_list)\n",
    "    sentences_padded = pad_sequences(tokenized_sentences, maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "    # Convert tags to numerical labels\n",
    "    t_labels = [[tag_map[label] for label in sentence_labels] for sentence_labels in labels_list]\n",
    "\n",
    "    # Padding for tags\n",
    "    labels_padded = pad_sequences(t_labels, maxlen=MAX_LEN, padding='post', value=tag_map[\"O\"])\n",
    "    labels_padded = np.array(labels_padded, dtype=np.int32)\n",
    "\n",
    "    return np.array(sentences_padded), labels_padded\n",
    "\n",
    "\n",
    "# Split into train (80%), validation (10%), and test (10%)\n",
    "train_sentences, temp_sentences, train_labels, temp_labels = train_test_split(\n",
    "    sentences_list, labels_list, test_size=0.2, random_state=42\n",
    ")\n",
    "val_sentences, test_sentences, val_labels, test_labels = train_test_split(\n",
    "    temp_sentences, temp_labels, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenize and pad data\n",
    "t_sentences, t_labels = get_params(train_sentences, train_labels)\n",
    "v_sentences, v_labels = get_params(val_sentences, val_labels)\n",
    "test_sentences, test_labels = get_params(test_sentences, test_labels)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((t_sentences, t_labels)).shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((v_sentences, v_labels)).batch(BATCH_SIZE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels)).batch(BATCH_SIZE)\n",
    "\n",
    "# Build the NER model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_LEN),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=RNN_UNITS, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=RNN_UNITS, return_sequences=True)),\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax'))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=15)\n",
    "\n",
    "\n",
    "# Function to predict entity tags\n",
    "def predict_entities(sentence):\n",
    "    reverse_tag_map = {v: k for k, v in tag_map.items()}\n",
    "\n",
    "    # Tokenize sentence\n",
    "    sentence_seq = tokenizer.texts_to_sequences([sentence.split()])\n",
    "    sentence_padded = pad_sequences(sentence_seq, maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "    # Predict\n",
    "    predictions = model.predict(sentence_padded)[0]\n",
    "\n",
    "    # Get tags\n",
    "    predicted_tags = [reverse_tag_map[np.argmax(word_pred)] for word_pred in predictions[:len(sentence.split())]]\n",
    "\n",
    "    return list(zip(sentence.split(), predicted_tags))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 samples\n",
      "Unique categories: ['sheep', 'butterfly', 'cow', 'spider', 'dog', 'hen', 'elephant', 'panda', 'horse', 'cat', 'squirrel', 'monkey']\n",
      "Example category mapping: {'sheep': np.int64(9), 'butterfly': np.int64(0), 'cow': np.int64(2), 'spider': np.int64(10), 'dog': np.int64(3), 'hen': np.int64(5), 'elephant': np.int64(4), 'panda': np.int64(8), 'horse': np.int64(6), 'cat': np.int64(1), 'squirrel': np.int64(11), 'monkey': np.int64(7)}\n",
      "Training size: 800, Testing size: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\тестове лютий зимове стажування\\env\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/30\n",
      "25/25 - 21s - 859ms/step - accuracy: 0.0975 - loss: 2.4839 - val_accuracy: 0.0650 - val_loss: 2.4856\n",
      "Epoch 2/30\n",
      "25/25 - 1s - 51ms/step - accuracy: 0.1163 - loss: 2.4755 - val_accuracy: 0.0650 - val_loss: 2.4825\n",
      "Epoch 3/30\n",
      "25/25 - 1s - 49ms/step - accuracy: 0.1350 - loss: 2.4278 - val_accuracy: 0.0800 - val_loss: 2.3982\n",
      "Epoch 4/30\n",
      "25/25 - 1s - 42ms/step - accuracy: 0.2062 - loss: 2.1898 - val_accuracy: 0.2050 - val_loss: 2.1168\n",
      "Epoch 5/30\n",
      "25/25 - 1s - 40ms/step - accuracy: 0.2500 - loss: 1.9580 - val_accuracy: 0.2950 - val_loss: 1.8356\n",
      "Epoch 6/30\n",
      "25/25 - 1s - 52ms/step - accuracy: 0.3587 - loss: 1.7012 - val_accuracy: 0.4200 - val_loss: 1.6130\n",
      "Epoch 7/30\n",
      "25/25 - 1s - 43ms/step - accuracy: 0.4000 - loss: 1.5102 - val_accuracy: 0.4650 - val_loss: 1.3702\n",
      "Epoch 8/30\n",
      "25/25 - 1s - 44ms/step - accuracy: 0.4437 - loss: 1.2984 - val_accuracy: 0.5100 - val_loss: 1.1606\n",
      "Epoch 9/30\n",
      "25/25 - 1s - 53ms/step - accuracy: 0.5537 - loss: 1.1065 - val_accuracy: 0.6600 - val_loss: 0.9876\n",
      "Epoch 10/30\n",
      "25/25 - 1s - 53ms/step - accuracy: 0.5863 - loss: 1.0200 - val_accuracy: 0.7150 - val_loss: 0.8481\n",
      "Epoch 11/30\n",
      "25/25 - 1s - 38ms/step - accuracy: 0.6313 - loss: 0.8875 - val_accuracy: 0.6650 - val_loss: 0.8242\n",
      "Epoch 12/30\n",
      "25/25 - 1s - 57ms/step - accuracy: 0.7113 - loss: 0.7839 - val_accuracy: 0.8350 - val_loss: 0.6934\n",
      "Epoch 13/30\n",
      "25/25 - 1s - 56ms/step - accuracy: 0.7362 - loss: 0.6916 - val_accuracy: 0.8050 - val_loss: 0.5934\n",
      "Epoch 14/30\n",
      "25/25 - 1s - 44ms/step - accuracy: 0.7600 - loss: 0.5930 - val_accuracy: 0.8250 - val_loss: 0.4833\n",
      "Epoch 15/30\n",
      "25/25 - 2s - 66ms/step - accuracy: 0.7800 - loss: 0.5269 - val_accuracy: 0.9000 - val_loss: 0.4334\n",
      "Epoch 16/30\n",
      "25/25 - 1s - 53ms/step - accuracy: 0.7950 - loss: 0.4879 - val_accuracy: 0.9100 - val_loss: 0.3970\n",
      "Epoch 17/30\n",
      "25/25 - 2s - 65ms/step - accuracy: 0.8112 - loss: 0.4615 - val_accuracy: 0.9250 - val_loss: 0.3613\n",
      "Epoch 18/30\n",
      "25/25 - 1s - 52ms/step - accuracy: 0.8537 - loss: 0.3953 - val_accuracy: 0.9350 - val_loss: 0.3415\n",
      "Epoch 19/30\n",
      "25/25 - 1s - 51ms/step - accuracy: 0.8863 - loss: 0.3471 - val_accuracy: 1.0000 - val_loss: 0.2525\n",
      "Epoch 20/30\n",
      "25/25 - 1s - 48ms/step - accuracy: 0.8988 - loss: 0.3420 - val_accuracy: 0.9750 - val_loss: 0.2403\n",
      "Epoch 21/30\n",
      "25/25 - 3s - 101ms/step - accuracy: 0.9413 - loss: 0.2585 - val_accuracy: 1.0000 - val_loss: 0.1500\n",
      "Epoch 22/30\n",
      "25/25 - 1s - 55ms/step - accuracy: 0.9638 - loss: 0.1912 - val_accuracy: 1.0000 - val_loss: 0.1232\n",
      "Epoch 23/30\n",
      "25/25 - 2s - 60ms/step - accuracy: 0.9650 - loss: 0.1829 - val_accuracy: 1.0000 - val_loss: 0.0929\n",
      "Epoch 24/30\n",
      "25/25 - 1s - 38ms/step - accuracy: 0.9762 - loss: 0.1522 - val_accuracy: 1.0000 - val_loss: 0.0734\n",
      "Epoch 25/30\n",
      "25/25 - 2s - 63ms/step - accuracy: 0.9900 - loss: 0.1329 - val_accuracy: 1.0000 - val_loss: 0.0633\n",
      "Epoch 26/30\n",
      "25/25 - 1s - 50ms/step - accuracy: 0.9875 - loss: 0.1057 - val_accuracy: 1.0000 - val_loss: 0.0496\n",
      "Epoch 27/30\n",
      "25/25 - 2s - 66ms/step - accuracy: 0.9887 - loss: 0.1047 - val_accuracy: 1.0000 - val_loss: 0.0396\n",
      "Epoch 28/30\n",
      "25/25 - 1s - 58ms/step - accuracy: 0.9925 - loss: 0.0877 - val_accuracy: 1.0000 - val_loss: 0.0348\n",
      "Epoch 29/30\n",
      "25/25 - 1s - 55ms/step - accuracy: 0.9887 - loss: 0.0877 - val_accuracy: 1.0000 - val_loss: 0.0284\n",
      "Epoch 30/30\n",
      "25/25 - 2s - 67ms/step - accuracy: 0.9962 - loss: 0.0687 - val_accuracy: 1.0000 - val_loss: 0.0234\n",
      "Training complete!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Predicted category: cow\n",
      "Model and tokenizer saved!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load JSON dataset\n",
    "with open(\"dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    datastore = json.load(f)\n",
    "\n",
    "# Initialize lists\n",
    "sentences = []\n",
    "categories = []\n",
    "\n",
    "# Load texts and labels\n",
    "for item in datastore:\n",
    "    sentences.append(item['sentence'])\n",
    "    categories.append(item['category'])\n",
    "\n",
    "print(f\"Loaded {len(sentences)} samples\")\n",
    "\n",
    "unique_categories = list(set(categories))\n",
    "print(f\"Unique categories: {unique_categories}\")\n",
    "\n",
    "# Encode labels into numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(categories)  \n",
    "\n",
    "# Check encoding\n",
    "print(f\"Example category mapping: {dict(zip(unique_categories, label_encoder.transform(unique_categories)))}\")\n",
    "\n",
    "# Tokenization parameters\n",
    "vocab_size = 10000\n",
    "max_length = 32\n",
    "embedding_dim = 16\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Convert texts into numerical sequences\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# Convert labels to a numpy array\n",
    "encoded_labels = np.array(encoded_labels, dtype=np.int32)\n",
    "\n",
    "# Split data into training and testing sets \n",
    "training_size = int(len(sentences) * 0.8)  # 80% for training, 20% for testing\n",
    "training_sentences = padded_sequences[:training_size]\n",
    "testing_sentences = padded_sequences[training_size:]\n",
    "\n",
    "training_labels = encoded_labels[:training_size]\n",
    "testing_labels = encoded_labels[training_size:]\n",
    "\n",
    "print(f\"Training size: {len(training_sentences)}, Testing size: {len(testing_sentences)}\")\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n",
    "    tf.keras.layers.Dense(48, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),  # Adding Dropout for regularization\n",
    "    tf.keras.layers.Dense(len(unique_categories), activation='softmax')  # Multi-class classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=8,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=False,\n",
    "    start_from_epoch=0\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())\n",
    "\n",
    "# Train the model \n",
    "num_epochs = 30\n",
    "history = model.fit(training_sentences, training_labels, epochs=num_epochs, validation_data=(testing_sentences, testing_labels), verbose=2)\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Function to predict the category of an animal \n",
    "def predict_animal(sentence):\n",
    "    sequence = tokenizer.texts_to_sequences([sentence])\n",
    "    padded = pad_sequences(sequence, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "    prediction = model.predict(padded)\n",
    "    predicted_label = np.argmax(prediction)\n",
    "    return label_encoder.inverse_transform([predicted_label])[0]\n",
    "\n",
    "# Example prediction\n",
    "test_sentence = \"There might be some cat in the picture, am I right?\"\n",
    "print(f\"Predicted category: {predict_animal(test_sentence)}\")\n",
    "\n",
    "# Save the model in Keras format\n",
    "model.save(\"animal_classifier_nlp.keras\")\n",
    "\n",
    "# Save the tokenizer\n",
    "import json\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open(\"tokenizer.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "\n",
    "print(\"Model and tokenizer saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
